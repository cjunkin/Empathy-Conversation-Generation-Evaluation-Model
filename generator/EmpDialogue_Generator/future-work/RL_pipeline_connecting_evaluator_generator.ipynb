{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Establishing RL Pipeline Between Evaluator and Generator**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "TeNfwAqcv1Cm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal is to achieve the following pipeline:\n",
        "1. Input tensor list of queries\n",
        "2. Output tensor list of responses\n",
        "3. Convert to csv for input to evaluator\n",
        "4. Pass through evaluator to obtain output csv with evaluator scores.\n",
        "5. Parse output csv to extract scores.\n",
        "6. Utilize scores to compute reward\n",
        "7. Push input, output, and reward tensors to trl to perform train step.\n",
        "\n",
        "Edit: I've achieved this pipeline with individual queries, goal is now to do this in batches so that it's at least reasonably fast. Currently at ~1 minute/iteration\n"
      ],
      "metadata": {
        "id": "xTy0IIEZQNXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "def construct_output_csv(fileName, responses):\n",
        "    with open(fileName, 'w', newline='') as csvfile:\n",
        "        fieldnames = ['id', 'seeker_post', 'response_post']\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for i in range(len(input_queries)):\n",
        "            writer.writerow({'id':f\"{i+1}\",'seeker_post':str(input_queries[i]), 'response_post':str(responses[i])})\n"
      ],
      "metadata": {
        "id": "jSxlZqEjn_8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Imports and Definitions\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model\n",
        "from trl.core import respond_to_batch\n",
        "\n",
        "# get models\n",
        "model = AutoModelForCausalLMWithValueHead.from_pretrained('gpt2')\n",
        "model_ref = create_reference_model(model)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "# initialize trainer\n",
        "ppo_config = {\"mini_batch_size\": 1, \"batch_size\": 1}\n",
        "config = PPOConfig(**ppo_config)\n",
        "ppo_trainer = PPOTrainer(config, model, model_ref, tokenizer)"
      ],
      "metadata": {
        "id": "VD2Cwu7uAqMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of steps 1,2, and 7 in the pipeline. We use a constant value for reward function."
      ],
      "metadata": {
        "id": "F-dR0NfDRxwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#example of one single train iteration\n",
        "query_txt = input_queries[0]\n",
        "query_tensor = tokenizer.encode(query_txt, return_tensors=\"pt\").cuda()\n",
        "\n",
        "# get model response\n",
        "response_tensor  = respond_to_batch(model, query_tensor)\n",
        "\n",
        "# reward -- just a constant for this example\n",
        "reward = [torch.tensor(1.0)]\n",
        "\n",
        "# train model for one step with ppo\n",
        "train_stats = ppo_trainer.step([query_tensor[0]], [response_tensor[0]], reward)"
      ],
      "metadata": {
        "id": "4nMeM_8uAj1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "try:\n",
        "\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "\n",
        "    DRIVE_PATH = '/content/gdrive/My\\ Drive/CS247-Empathy-Mental-Health'\n",
        "    DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n",
        "    if not os.path.exists(DRIVE_PYTHON_PATH):\n",
        "      %mkdir $DRIVE_PATH\n",
        "\n",
        "    ## the space in `My Drive` causes some issues,\n",
        "    ## make a symlink to avoid this\n",
        "    # Solved -> symlink for convenience\n",
        "    SYM_PATH = '/content/CS247-Empathy-Mental-Health'\n",
        "    if not os.path.exists(SYM_PATH):\n",
        "      !ln -s $DRIVE_PATH $SYM_PATH\n",
        "\n",
        "    running_in_colab = True\n",
        "\n",
        "    # We already mounted in our google drive.\n",
        "    # Enter the foler where you put files in:\n",
        "    %cd '/content/CS247-Empathy-Mental-Health'\n",
        "\n",
        "    # What files are there:\n",
        "    !ls\n",
        "\n",
        "\n",
        "except ModuleNotFoundError:\n",
        "    running_in_colab = False\n",
        "    print(\n",
        "        \"I guess you are running locally. If you get this message in Colab, check the files.\"\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmcyHw5CDedx",
        "outputId": "609a00aa-6095-4736-af96-662c56f22658"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/.shortcut-targets-by-id/1qwurxfG3wTYT_VY1AQ0AaMBt4LPZf23w/CS247-Empathy-Mental-Health\n",
            " best_emotion.pt\t   NaiveBaselineModel.ipynb\t   rlhf_q_2\n",
            " checkpoint_other_131.pt   output\t\t\t   rlhf_q_3\n",
            " checkpoint_other_79.pt    PretrainedModelQuerying.ipynb   rlhf_question_0_100\n",
            " Empathy-Mental-Health\t   rlhf_default_0_100\t\t   rlhf_question_0_200\n",
            " EmpDialogue_RecEC\t   rlhf_default_0_200\t\t   rlhf_question_0_300\n",
            " ER-reddit-test.csv\t   rlhf_default_logs.json\t   rlhf_question_logs.json\n",
            "'Generative Model.ipynb'   rlhf_length_0\t\t   rlhf_therapist_length_0_100\n",
            " glove.6B.100d.txt\t   rlhf_length_0_100\t\t   rlhf_therapist_length_0_200\n",
            " glove.6B.200d.txt\t   rlhf_length_0_200\t\t   rlhf_therapist_length_0_300\n",
            " glove.6B.300d.txt\t   rlhf_length_0_300\t\t   rlhf_therapist_length_logs.json\n",
            " glove.6B.50d.txt\t   rlhf_length_logs.json\t  'RL Training.ipynb'\n",
            " hard-gate-test.gdoc\t  'RLHF on SFT'\t\t\t   roberta-large.tsv\n",
            " hard-gate-test.txt\t   rlhf_q_1\t\t\t   SFT_GPT2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Empathy-Mental-Health/\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNC8QA98DrKE",
        "outputId": "784756d1-e455-4795-a896-f899815d0320"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'Empathy-Mental-Health/'\n",
            "/content/gdrive/.shortcut-targets-by-id/1eUJZcBYmEsh0qtMhIO6uDeGrZPPBmQio/Empathy-Mental-Health\n",
            "'Althoff academic license.docx'\t\t  dataset   README.md\t       src\n",
            "'Althoff attribution only license.docx'   output    requirements.txt   train.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This helps setup steps 3-6 in the pipeline. All functions for converting/parsing csv, extracting reward value are here."
      ],
      "metadata": {
        "id": "qfLqHESQR8Ap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_csv_for_evaluator(fileName, query, response):\n",
        "    with open(fileName, 'w', newline='') as csvfile:\n",
        "        fieldnames = ['id', 'seeker_post', 'response_post']\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        writer.writerow({'id':'1','seeker_post':str(query), 'response_post':str(response)})\n",
        "\n",
        "def calculate_reward_function_using_evaluator():\n",
        "    !python3 src/test.py \\\n",
        "\t--input_path /content/evaluator_input.csv \\\n",
        "\t--output_path /content/evaluator_output.csv \\\n",
        "\t--ER_model_path output/reddit_ER.pth \\\n",
        "\t--IP_model_path output/reddit_IP.pth \\\n",
        "\t--EX_model_path output/reddit_EX.pth\n",
        "    return\n",
        "\n",
        "#Algorithm to compute reward. Currently just summing up all 3 labels and normalizing to this scale:\n",
        "#6->3, 5->2, 4->1, 3->0, 2->-1, 1->-2, 0->-3\n",
        "def compute_reward_single_value(ER_label, IP_label, EX_label):\n",
        "    return [torch.tensor((ER_label + IP_label + EX_label) - 3, dtype=torch.float32)]\n",
        "\n",
        "\n",
        "def extract_reward_from_output_csv_from_evaluator_single_value():\n",
        "    input_df = pd.read_csv('/content/evaluator_output.csv', header=0)\n",
        "    ER_label = int(input_df.ER_label.astype(str).tolist()[0])\n",
        "    IP_label = int(input_df.IP_label.astype(str).tolist()[0])\n",
        "    EX_label = int(input_df.EX_label.astype(str).tolist()[0])\n",
        "    return compute_reward_single_value(ER_label, IP_label, EX_label)\n"
      ],
      "metadata": {
        "id": "Z5SWULcTFDgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps 1-7 for a single input query:"
      ],
      "metadata": {
        "id": "Vol-z8hpYD9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_entry = input_queries[0]\n",
        "query_tensor = tokenizer.encode(chat_entry, return_tensors=\"pt\").cuda()\n",
        "\n",
        "# get model response\n",
        "response_tensor  = respond_to_batch(model, query_tensor)\n",
        "response = tokenizer.decode(response_tensor[0], skip_special_tokens=True)\n",
        "\n",
        "# use query, response to obtain evaluator reward score\n",
        "convert_csv_for_evaluator('/content/evaluator_input.csv', chat_entry, response)\n",
        "calculate_reward_function_using_evaluator()\n",
        "reward = extract_reward_from_output_csv_from_evaluator_single_value()\n",
        "# # train step based on query, resonse, reward\n",
        "train_stats = ppo_trainer.step([query_tensor[0]], [response_tensor[0]], reward)\n",
        "\n",
        "print(\"query: \", chat_entry)\n",
        "print(\"response: \", response)\n",
        "print(\"reward\", reward)"
      ],
      "metadata": {
        "id": "c4AkYEslNTdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code will train using ppo on all of the queries in the list \"input_queries\". However, it does so only one query at a time, so we have to establish the full pipeline for each query. Thus, very very slow. About 1 min per query."
      ],
      "metadata": {
        "id": "-iNw5_ZpRdGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for chat_entry in input_queries:\n",
        "    print(\"query: \", chat_entry)\n",
        "    query_tensor = tokenizer.encode(chat_entry, return_tensors=\"pt\").cuda()\n",
        "\n",
        "    # get model response\n",
        "    response_tensor  = respond_to_batch(model, query_tensor)\n",
        "    response = tokenizer.decode(response_tensor[0], skip_special_tokens=True)\n",
        "    print(\"response: \", response)\n",
        "\n",
        "    # use query, response to obtain evaluator reward score\n",
        "    convert_csv_for_evaluator('/content/evaluator_input.csv', chat_entry, response)\n",
        "    calculate_reward_function_using_evaluator()\n",
        "    print(\"done forwarding through evaluator\")\n",
        "    reward = extract_reward_from_output_csv_from_evaluator_single_value()\n",
        "    print(\"reward\", reward)\n",
        "    # # train step based on query, resonse, reward\n",
        "    train_stats = ppo_trainer.step([query_tensor[0]], [response_tensor[0]], reward)"
      ],
      "metadata": {
        "id": "gqhnSRGCE7av",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04702c3e-bb87-4114-bb23-7c111509a42f"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "query:  Help. Help me. I dunno what I'm doing anymore\n",
            "response:  . Just get this boy kicked out of my god-shitclaw. Photo Archives Prev Next Field Administrator\n",
            "2024-03-08 23:02:08.896714: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-08 23:02:08.896778: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-08 23:02:08.903163: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-08 23:02:10.863083: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "done forwarding through evaluator\n",
            "reward [tensor(-3.)]\n",
            "query:  I'm done saying I love you to her because I don't want to hear a lie back to me. I feel so ugly and unwanted and unloved\n",
            "response:   and unloved. I'm still not even done saying this what is wrong to him right?\n",
            "\n",
            "2024-03-08 23:03:20.094110: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-08 23:03:20.094157: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-08 23:03:20.100491: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-08 23:03:22.571961: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "done forwarding through evaluator\n",
            "reward [tensor(-3.)]\n",
            "query:  Always feel like I'm being criticized and mocked even when alone.... Does anyone else experience this?\n",
            "response:   It's a long stretch of time, a long development. You'd be shocked at how long you\n",
            "2024-03-08 23:04:30.890777: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-08 23:04:30.890833: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-08 23:04:30.897697: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-08 23:04:32.878441: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "done forwarding through evaluator\n",
            "reward [tensor(-2.)]\n",
            "query:  My diet becomes fucked when i get depressed.. I can't control myself in the grocery store when I'm feeling down. Chips, cookies, soda, cake, you name It. Anyone else a stress eater?\n",
            "response:   I still feel like that. I'm but damn. I'm e.g. pulsating T\n",
            "2024-03-08 23:05:41.460089: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-08 23:05:41.460140: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-08 23:05:41.465896: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-08 23:05:43.559823: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "done forwarding through evaluator\n",
            "reward [tensor(-1.)]\n",
            "query:  I hate not knowing why. I was diagnosed with depression and anxiety when I was 14. That's almost 4 years ago and I still don't know why I am like this. I have more than enough, yet there are moments where I feel like there's no hope nor a bright future waiting for me. I feel alone and abandoned even though I have friends who know that I am depressed and am like this sometimes. I get so frustrated when I can't find out why I just can't be normal. Everything was alright about 2 hours ago, but it just changes so quickly and hard that I almost feel suicidal. I'm a fucking mess...\n",
            "response:   I just lie and only just keep coming back and thinking. I'm not giving up and the only\n",
            "2024-03-08 23:06:51.277982: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-08 23:06:51.278035: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-08 23:06:51.283956: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-08 23:06:53.202418: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "done forwarding through evaluator\n",
            "reward [tensor(1.)]\n",
            "query:  Not arsed with school. Can't study or do homework.. My family is in a bad way right now and I'm in a bad way, mentally. I just can't care about school anymore. Feels hopeless\n",
            "response:  . I just can't care about this. No.\n",
            "Showcase\n",
            "Column self best.\n",
            "\n",
            "2024-03-08 23:08:02.459943: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-08 23:08:02.459994: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-08 23:08:02.465929: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-08 23:08:04.510140: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "done forwarding through evaluator\n",
            "reward [tensor(-1.)]\n",
            "query:  My parents wont allow me to sick medical assistance because they think my depression is bs. Title. I'm 16, got no money. What to do?\n",
            "response:   School __________ District of California... female\n",
            "\n",
            "Dir. Jack Hancock Amazing fund tbh -\n",
            "2024-03-08 23:09:12.359552: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-08 23:09:12.359602: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-08 23:09:12.366609: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-08 23:09:14.514144: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "done forwarding through evaluator\n",
            "reward [tensor(-3.)]\n",
            "query:  I called in sick to work today. For the first time in a couple months I was feeling so Shitty I called into both my jobs today. Just feeling too burnt out, but I slept a lot, read quite a bit and finally managed to shower so the day a complete waste\n",
            "response:   of time. I worked hard, watched many times—read too much, a lot. I read\n",
            "2024-03-08 23:10:23.100202: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-08 23:10:23.100251: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-08 23:10:23.106186: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-08 23:10:25.058388: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "done forwarding through evaluator\n",
            "reward [tensor(-1.)]\n",
            "query:  Lost my job, have no reason to live. My career is over, my personal relationships are barely existent, and the friends I do have live far away.  I have absolutely no reason to live, no job prospects, no way to afford treatment. I will be killing myself soon unless a miracle happens.\n",
            "response:    I'm in a state of zero. I've just spent my life if I can't afford\n",
            "2024-03-08 23:11:33.736916: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-08 23:11:33.736973: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-08 23:11:33.743581: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-08 23:11:36.140491: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "done forwarding through evaluator\n",
            "reward [tensor(-1.)]\n",
            "query:  i wanna jump from an elevated place. i wanna kill myself, like really really would like to jump off an elevated place, but not die what do i do? drugs? fall from somewhere not far above the ground(like a 4meters jump)? you know, i'd just like to die but i have a gr8 life other than that (I have different lives in two different countries, just want to kill myself in one)\n",
            "response:   i i have been living my life so long and so there is no more worthy of my life (\n",
            "2024-03-08 23:12:46.559547: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-08 23:12:46.559606: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-08 23:12:46.565795: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-08 23:12:48.621708: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "done forwarding through evaluator\n",
            "reward [tensor(-1.)]\n",
            "query:  Wrong diagnosis-ADHD vs Bipolar Depression. For the past 5 years I've dealt with being misdiagnosed. I have ADHD and Depression but was diagnosed with Bipolar disorder. Has anyone else had this happen? What were the ramifications? How did you handle and what actions did you take?\n",
            "response:   I'm sorry. I'm sorry. I'm. Orion.ography. Over-training. Psychological\n",
            "2024-03-08 23:13:58.045428: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-08 23:13:58.045482: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-08 23:13:58.053105: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-08 23:14:00.061098: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "done forwarding through evaluator\n",
            "reward [tensor(1.)]\n",
            "query:  How long is this going to go for?. I wish I fucking knew. Fuck you and everyone else.\n",
            "response:   oh god fuck you and me. I wonder. I know I know. I know. I don\n",
            "2024-03-08 23:15:08.483663: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-08 23:15:08.483707: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-08 23:15:08.490264: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-08 23:15:10.445632: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "done forwarding through evaluator\n",
            "reward [tensor(-2.)]\n",
            "query:  I've had a hard time going back to school. Going back to school has been difficult for me, It's been around five years since I've been in a class room and I ask the teachers and the people that I go to my classes to cut me some slack for being rusty but they tell me that it doesn't matter, and my step mom doesn't want me to use the computer because she doesn't want to share. Its lead me to skipping classes because I feel like theirs no use in going to school, it would be cool to know how you would handle this situation and if you can give me any advice.\n",
            "response:   - - - - - - - - - - - - - - - - - - - -\n",
            "2024-03-08 23:16:17.828712: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-08 23:16:17.828778: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-08 23:16:17.835508: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-08 23:16:20.130148: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "done forwarding through evaluator\n",
            "reward [tensor(-3.)]\n",
            "query:  Depression. So any non med for depression ?\n",
            "response:  . It do not states the pigs. This way, mutual respect is earned and strengthened never to starve\n",
            "2024-03-08 23:17:31.212823: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-08 23:17:31.212874: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-08 23:17:31.219560: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-08 23:17:33.283523: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "done forwarding through evaluator\n",
            "reward [tensor(-3.)]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -1.46 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query:  All the people who will be kissed on New year's eve. I'll be alone like usual I'll never get someone to kiss me It's fine No one understands how hopelessly, alone, and angrily some people live. It would scare some attractive people that haven't ever lived like that.\n",
            "response:   It's beautiful. And it's means that you can, me you dare, I can multiply desires\n",
            "2024-03-08 23:18:41.517798: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-08 23:18:41.517856: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-08 23:18:41.523580: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-08 23:18:43.529253: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "done forwarding through evaluator\n",
            "reward [tensor(-2.)]\n",
            "query:  It‚Äôs terrible being alone in a crisis. Just wanted to express this.\n",
            "response:   I played. I beat. I has. It was.\n",
            "I must. But I know.\n",
            "2024-03-08 23:19:53.188250: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-08 23:19:53.188300: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-08 23:19:53.195362: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-08 23:19:55.437851: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "done forwarding through evaluator\n",
            "reward [tensor(-1.)]\n",
            "query:  honestly what is in this world for me.. nothing thats what i feel like living to make other people happy when they only like me cause they feel sorry for me not cause they actually like me jesus i live such a sorry life.\n",
            "response:   blah it am i explain. jesssps gate hip bottom turnoff. shit.\n",
            "\n",
            "\n",
            "2024-03-08 23:21:05.278356: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-08 23:21:05.278408: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-08 23:21:05.284630: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-08 23:21:07.282832: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "done forwarding through evaluator\n",
            "reward [tensor(-3.)]\n",
            "query:  Why the fuck am I depressed. I don't even remember when or why but I just feel empty Like I'll be having a great time and suddenly I just want to go home and sleep It's so fuckin dumb I just want to be happy I'm not even sure why like is it because I'm a little overweight? Or stress? Or because I can't get a girlfriend? Because that's a stupid reason to be depressed Why am I doing this to myself How the fuck do I fix this Why do I write all this trash thinking people will actually read it\n",
            "response:  , when they come to me and I feel like I am the best damn purple if it's the\n",
            "2024-03-08 23:22:20.411054: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-08 23:22:20.411103: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-08 23:22:20.417692: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-08 23:22:22.430008: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "done forwarding through evaluator\n",
            "reward [tensor(-1.)]\n",
            "query:  The world is really pretty.. Pity, that I won't be there anymore to cherish it.\n",
            "response:   Emugar, when I am There is everyone, Everywhere, When I am 95 The Lid of\n",
            "2024-03-08 23:23:55.752638: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-08 23:23:55.752685: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-08 23:23:55.759529: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-08 23:23:57.776038: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "done forwarding through evaluator\n",
            "reward [tensor(-3.)]\n",
            "query:  Don't know what this is.... Not sure if this is the right place but here I go. I feel alone, I feel trapped in my mind, I feel like life is a cycle and I don't know what to do anymore. I have gained weight in the past 3 months, everyone has noticed it all my friends and family. I have never been depressed nor ever felt like this really, only up until maybe 5 months ago. The thing is that I have not had anything traumatic happen to me to make me feel like this. I've had family problems but I don't really see that as a problem i've dealt with it for many years. Is it possible to be depressed for no real reason? Are these symptoms of depression?\n",
            "response:   Is I not the same? Is my being like being in my life and I don't tell what\n",
            "2024-03-08 23:25:07.978666: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-08 23:25:07.978725: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-08 23:25:07.989540: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-08 23:25:10.446648: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "done forwarding through evaluator\n",
            "reward [tensor(-1.)]\n",
            "query:  Having kids is the most selfish yet selfless thing a person can do.. I just wish I never existed.\n",
            "response:  😑 I think. … I wanted to.I just.. I just.. I.I.\n",
            "2024-03-08 23:26:20.660799: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-08 23:26:20.660849: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-08 23:26:20.667713: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-08 23:26:22.689688: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "done forwarding through evaluator\n",
            "reward [tensor(-1.)]\n",
            "query:  Why does death make people sad?. a relief from the hell known as life\n",
            "response:  . main one of the maintasking let camping have mission. skipHomesworn. Gibber\n",
            "2024-03-08 23:27:34.227056: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-08 23:27:34.227107: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-08 23:27:34.233458: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-08 23:27:36.245573: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "done forwarding through evaluator\n",
            "reward [tensor(-3.)]\n",
            "query:  If anyone will listen. I'm in a bad place right now. I could really use a friend.\n",
            "response:   A dirt that was without fury. Vritstand. He stood and got rap with the woman's\n",
            "2024-03-08 23:28:43.849018: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-08 23:28:43.849070: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-08 23:28:43.855200: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-08 23:28:45.932542: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "done forwarding through evaluator\n",
            "reward [tensor(-3.)]\n",
            "query:  When everything is a challenge. Tying your shoes..eating..pulling your shirt over your head, even just walking. Everything is hard like you're climbing up a mountain. What I wouldn't give to see the world differently\n",
            "response:  . From...From mountains, I Can do you daily.From my...I am.I sleep\n",
            "2024-03-08 23:29:53.732971: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-08 23:29:53.733021: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-08 23:29:53.739709: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-08 23:29:55.733181: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "done forwarding through evaluator\n",
            "reward [tensor(0.)]\n",
            "query:  I don't wanna go back to a psych unit a third time.. I feel like I'm being pushed over the edge. I don't want to go back to the psych unit for the third time in a 6 month period. What do I do?\n",
            "response:   I's, I II.To.To.. and III gotuca.Sim_\n"
          ]
        }
      ]
    }
  ]
}